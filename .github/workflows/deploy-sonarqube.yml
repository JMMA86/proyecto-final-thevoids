name: Deploy SonarQube to Azure

on:
  push:
    branches: [main]
    paths:
      - "**.tf"
      - "**.yml"
      - ".github/workflows/**"
  workflow_dispatch:
    inputs:
      destroy_infrastructure:
        description: "Destruir infraestructura existente"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      force_recreate:
        description: "Forzar recreación de recursos existentes"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"

permissions:
  contents: read
  security-events: write

env:
  TF_VERSION: "1.6.0"
  ANSIBLE_VERSION: "2.15"

jobs:
  infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    outputs:
      public_ip: ${{ steps.terraform_outputs.outputs.public_ip }}
      sonarqube_url: ${{ steps.terraform_outputs.outputs.sonarqube_url }}
      ssh_command: ${{ steps.terraform_outputs.outputs.ssh_command }}
      deployment_success: ${{ steps.deployment_status.outputs.success }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Setup Python for Ansible
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Ansible
        run: |
          python -m pip install --upgrade pip
          pip install ansible
          pip install azure-cli

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Create Terraform variables file
        run: |
          cat > deploy/terraform/terraform.tfvars << EOF
          subscription_id     = "${{ secrets.AZURE_SUBSCRIPTION_ID }}"
          resource_group_name = "${{ vars.RESOURCE_GROUP_NAME || 'rg-sonarqube-chile' }}"
          location            = "${{ vars.AZURE_LOCATION || 'Chile Central' }}"
          vm_size             = "${{ vars.VM_SIZE || 'Standard_B2s' }}"
          admin_username      = "${{ vars.ADMIN_USERNAME || 'azureuser' }}"
          admin_password      = "${{ secrets.ADMIN_PASSWORD }}"
          allowed_cidr        = "${{ vars.ALLOWED_CIDR || '0.0.0.0/0' }}"
          environment         = "${{ vars.ENVIRONMENT || 'production' }}"
          EOF

      - name: Terraform Format Check
        run: |
          cd deploy/terraform
          terraform fmt -check=true -diff=true

      - name: Terraform Init
        run: |
          cd deploy/terraform
          terraform init

      - name: Terraform Validate
        run: |
          cd deploy/terraform
          terraform validate

      - name: Handle existing resources
        run: |
          cd deploy/terraform

          if [ "${{ github.event.inputs.force_recreate }}" = "true" ]; then
            echo "Force recreate enabled - removing existing resources..."
            
            # Eliminar recursos existentes con Azure CLI
            RG_EXISTS=$(az group exists --name "${{ vars.RESOURCE_GROUP_NAME || 'rg-sonarqube-chile' }}" --output tsv)
            if [ "$RG_EXISTS" = "true" ]; then
              echo "Deleting existing resource group..."
              az group delete --name "${{ vars.RESOURCE_GROUP_NAME || 'rg-sonarqube-chile' }}" --yes --no-wait
              
              # Esperar a que se complete la eliminación
              echo "Waiting for resource group deletion to complete..."
              while az group exists --name "${{ vars.RESOURCE_GROUP_NAME || 'rg-sonarqube-chile' }}" --output tsv | grep -q "true"; do
                echo "Still deleting... waiting 30 seconds"
                sleep 30
              done
              echo "Resource group deleted successfully"
            fi
          else
            echo "Attempting to import existing resources..."
            RG_NAME="${{ vars.RESOURCE_GROUP_NAME || 'rg-sonarqube-chile' }}"
            SUBSCRIPTION_ID="${{ secrets.AZURE_SUBSCRIPTION_ID }}"
            
            # Función para importar recurso si existe
            import_resource() {
              local resource_type="$1"
              local resource_name="$2"
              local terraform_resource="$3"
              local resource_id="$4"
              
              if [ ! -z "$resource_id" ] && [ "$resource_id" != "null" ]; then
                echo "Importing $resource_type '$resource_name'..."
                terraform import "$terraform_resource" "$resource_id" || echo "Import failed or resource already in state"
              fi
            }
            
            # Resource Group
            RG_EXISTS=$(az group exists --name "$RG_NAME" --output tsv)
            if [ "$RG_EXISTS" = "true" ]; then
              import_resource "Resource Group" "$RG_NAME" "azurerm_resource_group.main" "/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RG_NAME"
            fi
            
            # Virtual Network
            VNET_ID=$(az network vnet show --resource-group "$RG_NAME" --name "vnet-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            import_resource "Virtual Network" "vnet-sonarqube" "azurerm_virtual_network.main" "$VNET_ID"
            
            # Subnet
            SUBNET_ID=$(az network vnet subnet show --resource-group "$RG_NAME" --vnet-name "vnet-sonarqube" --name "internal" --query "id" --output tsv 2>/dev/null || echo "")
            import_resource "Subnet" "internal" "azurerm_subnet.internal" "$SUBNET_ID"
            
            # Network Security Group
            NSG_ID=$(az network nsg show --resource-group "$RG_NAME" --name "nsg-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            import_resource "Network Security Group" "nsg-sonarqube" "azurerm_network_security_group.main" "$NSG_ID"
            
            # Public IP
            PIP_ID=$(az network public-ip show --resource-group "$RG_NAME" --name "pip-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            import_resource "Public IP" "pip-sonarqube" "azurerm_public_ip.main" "$PIP_ID"
            
            # Network Interface
            NIC_ID=$(az network nic show --resource-group "$RG_NAME" --name "nic-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            import_resource "Network Interface" "nic-sonarqube" "azurerm_network_interface.main" "$NIC_ID"
            
            # Network Interface Security Group Association
            if [ ! -z "$NIC_ID" ] && [ ! -z "$NSG_ID" ]; then
              ASSOC_ID="${NIC_ID}|${NSG_ID}"
              import_resource "NIC-NSG Association" "nic-nsg-association" "azurerm_network_interface_security_group_association.main" "$ASSOC_ID"
            fi
            
            # Virtual Machine
            VM_ID=$(az vm show --resource-group "$RG_NAME" --name "vm-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            import_resource "Virtual Machine" "vm-sonarqube" "azurerm_linux_virtual_machine.main" "$VM_ID"
          fi
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        continue-on-error: true

      - name: Terraform Plan
        run: |
          cd deploy/terraform
          terraform plan -out=tfplan
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

      - name: Terraform Destroy (if requested)
        if: github.event.inputs.destroy_infrastructure == 'true'
        run: |
          echo "Destruyendo infraestructura existente..."
          cd deploy/terraform
          terraform destroy -auto-approve
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

      - name: Terraform Apply
        if: github.event.inputs.destroy_infrastructure != 'true'
        run: |
          cd deploy/terraform

          # Intentar aplicar el plan
          if ! terraform apply -auto-approve tfplan; then
            echo "Apply failed, possibly due to resource conflicts"
            echo "Attempting to resolve conflicts and retry..."
            
            # Importar recursos que pueden estar causando conflictos
            RG_NAME="${{ vars.RESOURCE_GROUP_NAME || 'rg-sonarqube-chile' }}"
            SUBSCRIPTION_ID="${{ secrets.AZURE_SUBSCRIPTION_ID }}"
            
            # Intentar importar recursos que falten después del primer fallo
            echo "Attempting to import additional resources..."
            
            # Virtual Network
            VNET_ID=$(az network vnet show --resource-group "$RG_NAME" --name "vnet-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            if [ ! -z "$VNET_ID" ]; then
              terraform import azurerm_virtual_network.main "$VNET_ID" 2>/dev/null || echo "VNET import skipped"
            fi
            
            # Network Security Group
            NSG_ID=$(az network nsg show --resource-group "$RG_NAME" --name "nsg-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            if [ ! -z "$NSG_ID" ]; then
              terraform import azurerm_network_security_group.main "$NSG_ID" 2>/dev/null || echo "NSG import skipped"
            fi
            
            # Subnet
            SUBNET_ID=$(az network vnet subnet show --resource-group "$RG_NAME" --vnet-name "vnet-sonarqube" --name "internal" --query "id" --output tsv 2>/dev/null || echo "")
            if [ ! -z "$SUBNET_ID" ]; then
              terraform import azurerm_subnet.internal "$SUBNET_ID" 2>/dev/null || echo "Subnet import skipped"
            fi
            
            # Network Interface
            NIC_ID=$(az network nic show --resource-group "$RG_NAME" --name "nic-sonarqube" --query "id" --output tsv 2>/dev/null || echo "")
            if [ ! -z "$NIC_ID" ]; then
              terraform import azurerm_network_interface.main "$NIC_ID" 2>/dev/null || echo "NIC import skipped"
            fi
            
            # Refresh state y planificar nuevamente
            terraform refresh
            terraform plan -out=tfplan-retry
            
            # Segundo intento de aplicación
            if ! terraform apply -auto-approve tfplan-retry; then
              echo "Second apply attempt failed"
              echo "Checking current state..."
              terraform state list
              exit 1
            fi
          fi
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

      - name: Get Terraform Outputs
        if: github.event.inputs.destroy_infrastructure != 'true'
        id: terraform_outputs
        run: |
          cd deploy/terraform
          echo "public_ip=$(terraform output -raw public_ip_address)" >> $GITHUB_OUTPUT
          echo "sonarqube_url=$(terraform output -raw sonarqube_url)" >> $GITHUB_OUTPUT
          echo "ssh_command=$(terraform output -raw ssh_connection_command)" >> $GITHUB_OUTPUT

          # Guardar clave SSH privada
          terraform output -raw ssh_private_key > ../../deploy/ansible/ssh_key.pem
          chmod 600 ../../deploy/ansible/ssh_key.pem

      - name: Upload SSH Key and Inventory
        if: github.event.inputs.destroy_infrastructure != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: deployment-artifacts
          path: |
            deploy/ansible/ssh_key.pem
          retention-days: 1

      - name: Set deployment status
        if: github.event.inputs.destroy_infrastructure != 'true'
        id: deployment_status
        run: echo "success=true" >> $GITHUB_OUTPUT

  configure-sonarqube:
    name: Configure SonarQube
    runs-on: ubuntu-latest
    needs: infrastructure
    if: github.event.inputs.destroy_infrastructure != 'true' && needs.infrastructure.outputs.deployment_success == 'true'
    outputs:
      sonarqube_ready: ${{ steps.sonarqube_status.outputs.ready }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python for Ansible
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Ansible
        run: |
          python -m pip install --upgrade pip
          pip install ansible

      - name: Download deployment artifacts
        uses: actions/download-artifact@v4
        with:
          name: deployment-artifacts
          path: deploy/ansible/

      - name: Set SSH key permissions
        run: chmod 600 deploy/ansible/ssh_key.pem

      - name: Create Ansible Inventory
        run: |
          cat > deploy/ansible/inventory.yml << EOF
          all:
            hosts:
              sonarqube-vm:
                ansible_host: ${{ needs.infrastructure.outputs.public_ip }}
                ansible_user: ${{ vars.ADMIN_USERNAME || 'azureuser' }}
                ansible_ssh_private_key_file: ./ssh_key.pem
                ansible_ssh_common_args: '-o StrictHostKeyChecking=no'
                ansible_python_interpreter: /usr/bin/python3
          EOF

      - name: Wait for VM to be ready
        run: |
          echo "Esperando que la VM esté lista..."
          for i in {1..20}; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -i deploy/ansible/ssh_key.pem ${{ vars.ADMIN_USERNAME || 'azureuser' }}@${{ needs.infrastructure.outputs.public_ip }} "echo 'VM ready'"; then
              echo "VM está lista!"
              break
            fi
            echo "Intento $i/20 - esperando 30 segundos..."
            sleep 30
          done

      - name: Fix Docker permissions and restart services
        run: |
          echo "Fixing Docker permissions and restarting Docker service..."
          ssh -o StrictHostKeyChecking=no -i deploy/ansible/ssh_key.pem ${{ vars.ADMIN_USERNAME || 'azureuser' }}@${{ needs.infrastructure.outputs.public_ip }} '
            # Stop Docker service
            sudo systemctl stop docker
            
            # Fix Docker socket permissions
            sudo chmod 666 /var/run/docker.sock || true
            
            # Add user to docker group
            sudo usermod -aG docker $USER
            
            # Start Docker service
            sudo systemctl start docker
            sudo systemctl enable docker
            
            # Wait for Docker to be ready
            sleep 10
            
            # Test Docker access
            if ! docker info >/dev/null 2>&1; then
              echo "Direct docker command failed, trying with sudo..."
              sudo docker info
            else
              echo "Docker is accessible!"
            fi
            
            # Clean up any existing containers
            sudo docker system prune -f || true
          '

      - name: Test Docker access and create wrapper script
        run: |
          echo "Testing Docker access and creating wrapper script..."
          ssh -o StrictHostKeyChecking=no -i deploy/ansible/ssh_key.pem ${{ vars.ADMIN_USERNAME || 'azureuser' }}@${{ needs.infrastructure.outputs.public_ip }} '
            # Test if user can run docker commands
            if docker ps >/dev/null 2>&1; then
              echo "Docker works without sudo"
              echo "#!/bin/bash" > /tmp/docker-compose-wrapper.sh
              echo "docker-compose \"\$@\"" >> /tmp/docker-compose-wrapper.sh
            else
              echo "Docker needs sudo, creating sudo wrapper"
              echo "#!/bin/bash" > /tmp/docker-compose-wrapper.sh
              echo "sudo docker-compose \"\$@\"" >> /tmp/docker-compose-wrapper.sh
            fi
            chmod +x /tmp/docker-compose-wrapper.sh
            
            # Also create docker wrapper
            if docker ps >/dev/null 2>&1; then
              echo "#!/bin/bash" > /tmp/docker-wrapper.sh
              echo "docker \"\$@\"" >> /tmp/docker-wrapper.sh
            else
              echo "#!/bin/bash" > /tmp/docker-wrapper.sh
              echo "sudo docker \"\$@\"" >> /tmp/docker-wrapper.sh
            fi
            chmod +x /tmp/docker-wrapper.sh
          '

      - name: Modify Ansible playbook to use sudo for Docker commands
        run: |
          cd deploy/ansible
          # Create a temporary modified playbook
          cp configure-sonarqube.yml configure-sonarqube-modified.yml

          # Replace docker-compose commands with sudo versions
          sed -i 's/docker-compose down/sudo docker-compose down/g' configure-sonarqube-modified.yml
          sed -i 's/docker-compose up -d/sudo docker-compose up -d/g' configure-sonarqube-modified.yml

          # Also handle any direct docker commands
          sed -i 's/docker /sudo docker /g' configure-sonarqube-modified.yml

      - name: Run Ansible Playbook
        id: ansible_run
        run: |
          cd deploy/ansible
          ansible-playbook -i inventory.yml configure-sonarqube-modified.yml -v
        env:
          ANSIBLE_HOST_KEY_CHECKING: false
        continue-on-error: true

      - name: Fallback - Deploy SonarQube directly with Docker
        if: steps.ansible_run.outcome == 'failure'
        run: |
          echo "Ansible failed, deploying SonarQube directly..."
          ssh -o StrictHostKeyChecking=no -i deploy/ansible/ssh_key.pem ${{ vars.ADMIN_USERNAME || 'azureuser' }}@${{ needs.infrastructure.outputs.public_ip }} '
            # Stop any existing SonarQube containers
            sudo docker stop sonarqube 2>/dev/null || true
            sudo docker rm sonarqube 2>/dev/null || true
            
            # Create necessary directories
            sudo mkdir -p /opt/sonarqube/data
            sudo mkdir -p /opt/sonarqube/extensions
            sudo mkdir -p /opt/sonarqube/logs
            sudo chown -R 999:999 /opt/sonarqube
            
            # Set required system settings
            sudo sysctl -w vm.max_map_count=262144
            echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
            
            # Run SonarQube container directly
            sudo docker run -d \
              --name sonarqube \
              -p 9000:9000 \
              -v /opt/sonarqube/data:/opt/sonarqube/data \
              -v /opt/sonarqube/extensions:/opt/sonarqube/extensions \
              -v /opt/sonarqube/logs:/opt/sonarqube/logs \
              -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true \
              sonarqube:latest
            
            # Wait for container to start
            sleep 30
            
            # Check if container is running
            sudo docker ps | grep sonarqube || {
              echo "SonarQube container failed to start, checking logs..."
              sudo docker logs sonarqube
              exit 1
            }
          '

      - name: Verify SonarQube is running
        id: sonarqube_status
        run: |
          echo "Verificando que SonarQube esté disponible..."
          SONARQUBE_READY=false

          # First check if Docker container is running
          echo "Checking Docker container status..."
          ssh -o StrictHostKeyChecking=no -i deploy/ansible/ssh_key.pem ${{ vars.ADMIN_USERNAME || 'azureuser' }}@${{ needs.infrastructure.outputs.public_ip }} '
            # Check if any SonarQube container exists and is running
            CONTAINER_ID=$(sudo docker ps -q --filter "name=sonarqube" 2>/dev/null || echo "")
            if [ ! -z "$CONTAINER_ID" ]; then
              echo "SonarQube container is running (ID: $CONTAINER_ID)"
              sudo docker logs $CONTAINER_ID --tail 20 2>/dev/null || echo "Could not retrieve logs"
            else
              echo "SonarQube container is not running"
              # Check if container exists but is stopped
              STOPPED_CONTAINER=$(sudo docker ps -a -q --filter "name=sonarqube" 2>/dev/null || echo "")
              if [ ! -z "$STOPPED_CONTAINER" ]; then
                echo "Found stopped SonarQube container, attempting to start it..."
                sudo docker start $STOPPED_CONTAINER || echo "Failed to start container"
                sleep 10
              else
                echo "No SonarQube container found at all"
                # List all containers for debugging
                echo "All containers:"
                sudo docker ps -a || echo "Could not list containers"
              fi
            fi
          '

          # Now check API availability
          for i in {1..20}; do
            echo "Checking API availability (attempt $i/20)..."
            if curl -f -s "${{ needs.infrastructure.outputs.sonarqube_url }}/api/system/status" | grep -q "UP"; then
              echo "SonarQube está funcionando correctamente!"
              SONARQUBE_READY=true
              break
            elif curl -s "${{ needs.infrastructure.outputs.sonarqube_url }}/api/system/status"; then
              echo "API responded but not UP yet, checking response..."
              curl -s "${{ needs.infrastructure.outputs.sonarqube_url }}/api/system/status"
            else
              echo "No response from API yet..."
            fi
            echo "Intento $i/20 - esperando 45 segundos..."
            sleep 45
          done

          if [ "$SONARQUBE_READY" = "false" ]; then
            echo "SonarQube no está disponible después de esperar. Verificando logs..."
            ssh -o StrictHostKeyChecking=no -i deploy/ansible/ssh_key.pem ${{ vars.ADMIN_USERNAME || 'azureuser' }}@${{ needs.infrastructure.outputs.public_ip }} '
              # Find SonarQube container and get logs
              CONTAINER_ID=$(sudo docker ps -a -q --filter "name=sonarqube" 2>/dev/null || echo "")
              if [ ! -z "$CONTAINER_ID" ]; then
                echo "Getting logs from container $CONTAINER_ID:"
                sudo docker logs $CONTAINER_ID --tail 50 2>/dev/null || echo "Could not retrieve container logs"
              else
                echo "No SonarQube container found for log retrieval"
                echo "Available containers:"
                sudo docker ps -a 2>/dev/null || echo "Could not list containers"
              fi
            '
          fi

          echo "ready=$SONARQUBE_READY" >> $GITHUB_OUTPUT

      - name: Display SonarQube Information
        run: |
          echo "SonarQube configurado exitosamente!"
          echo "=================================="
          echo "SonarQube URL: ${{ needs.infrastructure.outputs.sonarqube_url }}"
          echo "SSH Command: ${{ needs.infrastructure.outputs.ssh_command }}"
          echo "Credenciales por defecto:"
          echo "  Usuario: admin"
          echo "  Contraseña: admin"

  sonar-analysis:
    name: Run SonarQube Analysis
    runs-on: ubuntu-latest
    needs: [infrastructure, configure-sonarqube]
    if: github.event.inputs.destroy_infrastructure != 'true' && needs.configure-sonarqube.outputs.sonarqube_ready == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Shallow clones should be disabled for better analysis

      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: "17"
          distribution: "temurin"

      - name: Cache SonarQube packages
        uses: actions/cache@v3
        with:
          path: ~/.sonar/cache
          key: ${{ runner.os }}-sonar
          restore-keys: ${{ runner.os }}-sonar

      - name: Cache Maven packages
        uses: actions/cache@v3
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2

      - name: Wait for SonarQube to be fully ready
        run: |
          echo "Esperando a que SonarQube esté completamente listo para análisis..."
          for i in {1..10}; do
            if curl -f -s "${{ needs.infrastructure.outputs.sonarqube_url }}/api/system/health" | grep -q "GREEN"; then
              echo "SonarQube está listo para análisis!"
              break
            fi
            echo "Intento $i/10 - esperando 30 segundos..."
            sleep 30
          done

      - name: Generate SonarQube token
        id: sonar_token
        run: |
          # Crear token usando la API de SonarQube (usuario/contraseña por defecto: admin/admin)
          TOKEN_RESPONSE=$(curl -u admin:admin -X POST "${{ needs.infrastructure.outputs.sonarqube_url }}/api/user_tokens/generate" \
            -d "name=github-actions-$(date +%s)" 2>/dev/null || echo "")

          if echo "$TOKEN_RESPONSE" | grep -q "token"; then
            TOKEN=$(echo "$TOKEN_RESPONSE" | grep -o '"token":"[^"]*"' | cut -d'"' -f4)
            echo "sonar_token=$TOKEN" >> $GITHUB_OUTPUT
            echo "Token generado exitosamente"
          else
            echo "Error generando token, usando credenciales básicas"
            echo "sonar_token=" >> $GITHUB_OUTPUT
          fi

      - name: Run tests and generate coverage
        run: |
          # Fix Maven wrapper permissions
          chmod +x mvnw
          ./mvnw clean test jacoco:report

      - name: Run SonarQube analysis
        env:
          SONAR_TOKEN: ${{ steps.sonar_token.outputs.sonar_token }}
          SONAR_HOST_URL: ${{ needs.infrastructure.outputs.sonarqube_url }}
        run: |
          # Ensure Maven wrapper has execute permissions
          chmod +x mvnw

          if [ ! -z "$SONAR_TOKEN" ]; then
            echo "Ejecutando análisis con token..."
            ./mvnw sonar:sonar \
              -Dsonar.projectKey=proyecto-final-thevoids \
              -Dsonar.projectName="Proyecto Final TheVoids" \
              -Dsonar.host.url=$SONAR_HOST_URL \
              -Dsonar.token=$SONAR_TOKEN
          else
            echo "Ejecutando análisis con credenciales básicas..."
            ./mvnw sonar:sonar \
              -Dsonar.projectKey=proyecto-final-thevoids \
              -Dsonar.projectName="Proyecto Final TheVoids" \
              -Dsonar.host.url=$SONAR_HOST_URL \
              -Dsonar.login=admin \
              -Dsonar.password=admin
          fi

      - name: Install Trivy
        run: |
          # Install Trivy for vulnerability scanning
          sudo apt-get update
          sudo apt-get install wget apt-transport-https gnupg lsb-release
          wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
          echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
          sudo apt-get update
          sudo apt-get install trivy

      - name: Run Trivy vulnerability scan
        run: |
          echo "Ejecutando análisis de vulnerabilidades con Trivy..."

          # Create reports directory
          mkdir -p trivy-reports

          # Scan filesystem for vulnerabilities (code, configs, secrets)
          echo "Scanning filesystem for vulnerabilities..."
          trivy fs . \
            --format sarif \
            --output trivy-reports/trivy-fs-results.sarif \
            --severity HIGH,CRITICAL \
            --scanners vuln,secret,config

          # Scan filesystem for all vulnerabilities (human readable)
          echo "Generating detailed vulnerability report..."
          trivy fs . \
            --format table \
            --output trivy-reports/trivy-fs-report.txt \
            --severity LOW,MEDIUM,HIGH,CRITICAL \
            --scanners vuln,secret,config

          # Scan Maven dependencies
          echo "Scanning Maven dependencies..."
          if [ -f "pom.xml" ]; then
            trivy fs . \
              --format sarif \
              --output trivy-reports/trivy-maven-results.sarif \
              --severity HIGH,CRITICAL \
              --scanners vuln \
              --dependency-tree
          fi

          # Generate summary
          echo "TRIVY VULNERABILITY SCAN SUMMARY" > trivy-reports/summary.txt
          echo "=====================================" >> trivy-reports/summary.txt
          echo "Scan completed at: $(date)" >> trivy-reports/summary.txt
          echo "" >> trivy-reports/summary.txt

          # Count vulnerabilities by severity
          if [ -f "trivy-reports/trivy-fs-report.txt" ]; then
            echo "Vulnerability counts:" >> trivy-reports/summary.txt
            grep -c "HIGH" trivy-reports/trivy-fs-report.txt >> trivy-reports/summary.txt 2>/dev/null || echo "HIGH: 0" >> trivy-reports/summary.txt
            grep -c "CRITICAL" trivy-reports/trivy-fs-report.txt >> trivy-reports/summary.txt 2>/dev/null || echo "CRITICAL: 0" >> trivy-reports/summary.txt
            echo "" >> trivy-reports/summary.txt
          fi

          echo "Trivy scan completed!"

      - name: Upload Trivy scan results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: trivy-vulnerability-reports
          path: trivy-reports/
          retention-days: 30

      - name: Upload SARIF results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: trivy-reports/trivy-fs-results.sarif
          category: trivy-filesystem

      - name: Upload Maven SARIF results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always() && hashFiles('pom.xml') != ''
        with:
          sarif_file: trivy-reports/trivy-maven-results.sarif
          category: trivy-maven

      - name: Display Trivy results summary
        run: |
          echo "Trivy Vulnerability Scan Results"
          echo "===================================="

          if [ -f "trivy-reports/summary.txt" ]; then
            cat trivy-reports/summary.txt
          fi

          # Show critical and high vulnerabilities
          if [ -f "trivy-reports/trivy-fs-report.txt" ]; then
            echo ""
            echo "Critical and High Vulnerabilities:"
            echo "------------------------------------"
            grep -A 5 -B 1 "CRITICAL\|HIGH" trivy-reports/trivy-fs-report.txt | head -20 || echo "No critical/high vulnerabilities found!"
          fi

      - name: Wait for analysis to complete
        run: |
          echo "Esperando a que el análisis se complete..."
          sleep 30

          # Verificar que el proyecto aparezca en SonarQube
          for i in {1..10}; do
            if curl -u admin:admin -f -s "${{ needs.infrastructure.outputs.sonarqube_url }}/api/projects/search?projects=proyecto-final-thevoids" | grep -q "proyecto-final-thevoids"; then
              echo "Análisis completado y proyecto visible en SonarQube!"
              break
            fi
            echo "Intento $i/10 - esperando 15 segundos..."
            sleep 15
          done

      - name: Display analysis results
        run: |
          echo "Análisis de código y vulnerabilidades completado!"
          echo "===================================================="
          echo ""
          echo "SonarQube Analysis:"
          echo "Ver resultados en: ${{ needs.infrastructure.outputs.sonarqube_url }}/dashboard?id=proyecto-final-thevoids"
          echo ""
          echo "Trivy Security Scan:"
          echo "Los reportes de vulnerabilidades están disponibles en los artifacts del workflow"
          echo ""
          echo "Resumen:"
          echo "- Proyecto: proyecto-final-thevoids"
          echo "- SonarQube: ${{ needs.infrastructure.outputs.sonarqube_url }}"
          echo "- Security Scan: Trivy (ver artifacts para detalles)"

  cleanup:
    name: Cleanup temporary files
    runs-on: ubuntu-latest
    needs: [infrastructure, configure-sonarqube, sonar-analysis]
    if: always()
    steps:
      - name: Save deployment info
        if: needs.infrastructure.outputs.deployment_success == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: deployment-info
          path: |
            deploy/terraform/terraform.tfstate
          retention-days: 30

      - name: Remove sensitive artifacts
        uses: actions/github-script@v7
        with:
          script: |
            try {
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: 'deployment-artifacts'
              });
              console.log('Sensitive artifacts cleaned up');
            } catch (error) {
              console.log('No sensitive artifacts to clean up');
            }
